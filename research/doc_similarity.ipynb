{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9400868-ea6b-48cf-8b2a-e80ae069e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pypdf import PdfReader\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from razdel import sentenize\n",
    "import numpy as np\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from copy import deepcopy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f507b7-8a39-4c99-be4b-2170ba162cf1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5b20a60-7746-4c2b-996e-96054009906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files_cropped/docs_flattened.pickle', 'rb') as f:\n",
    "    all_docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a47bd121-f47f-4859-9038-baf11a120d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': 'ГОСТ ISO/TR 10993-33-2018',\n",
       " 'title': 'Изделия медицинские. Оценка биологического действия медицинских изделий. Часть 33. Руководство по испытаниям на генотоксичность. Дополнение к ISO 10993-3',\n",
       " 'status': 'active',\n",
       " 'date_start': '06.01.2019',\n",
       " 'date_cancel': '',\n",
       " 'replaced_by': '',\n",
       " 'OKS': '01.020',\n",
       " 'file_path': '',\n",
       " 'file_url': 'https://allgosts.ru/01/020/gost_iso!tr_10993-33-2018',\n",
       " 'OKS_main': '01 ОБЩИЕ ПОЛОЖЕНИЯ. ТЕРМИНОЛОГИЯ. СТАНДАРТИЗАЦИЯ. ДОКУМЕНТАЦИЯ',\n",
       " 'OKS_section': '01.020 Терминология (принципы и координация)',\n",
       " 'id': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2555f222-61a6-4f68-a828-f68279cbf931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(file):\n",
    "    text = ''\n",
    "    if not file['file_path']:\n",
    "        text = file['title']\n",
    "    else:\n",
    "        try:\n",
    "            reader = PdfReader('files_cropped/' + file['file_path'])\n",
    "            for page in reader.pages[1:-1]:\n",
    "                page_text = page.extract_text()\n",
    "                text += (' ' + page_text)\n",
    "            text = text \\\n",
    "                    .replace('\\xad\\n', '') \\\n",
    "                    .replace('\\n\\xad', '') \\\n",
    "                    .replace('\\n\\n', ' ') \\\n",
    "                    .replace('\\n', ' ') \\\n",
    "                    .replace('   ', ' ') \\\n",
    "                    .replace('  ', ' ')\n",
    "        except:\n",
    "            text = file['title']\n",
    "    return {\n",
    "        'gost_number': file['number'],\n",
    "        'title': file['title'],\n",
    "        'filename': file['file_path'],\n",
    "        'text': text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7120642b-fc8d-45a3-9574-aa855a04bfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b43f03582a74340b9f2c3fc575b7986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        docs_w_text = parallel(delayed(get_text)(doc) for doc in tqdm(all_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0127a46-34b3-421f-990c-28e086073512",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files_cropped/doc_with_text_for_similarity.pickle', 'wb') as f:\n",
    "    pickle.dump(docs_w_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19bc5b0e-8de3-4d41-bf91-7968578dc7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "punct = '!\"#$%&()*\\+,-\\./:;<=>?@\\[\\]^_`{|}~„“«»†*\\—/\\-‘’'\n",
    "stemmer = SnowballStemmer('russian')\n",
    "stopwords = stopwords.words('russian')\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f76759-7cd8-47f6-aed7-21c5e4938f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_doc(doc):\n",
    "    doc = deepcopy(doc)\n",
    "    text = doc['text'].lower()\n",
    "    words = [w.strip(punct) for w in word_tokenize(text)]\n",
    "    text = ' '.join([stemmer.stem(w) for w in words if w not in stopwords and w != ''])\n",
    "    doc['text_normalized'] = text\n",
    "    return doc\n",
    "\n",
    "def lemmatize_doc(doc):\n",
    "    doc = deepcopy(doc)\n",
    "    text = doc['text'].lower()\n",
    "    words = [w.strip(punct) for w in word_tokenize(text)]\n",
    "    words = [w for w in words if w not in stopwords and w != '']\n",
    "    doc['text_normalized'] = ' '.join([morph.parse(w)[0].normal_form for w in words])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9aa4b7-fa70-4ec2-a2e4-ee9aa7749519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tagged_doc(doc, index):\n",
    "    return TaggedDocument(words=stem_text(doc['text']), tags=[str(index)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "17ed36bf-5f0d-42e8-805f-7ac3019b78cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b188c8dda354656a038759e1239506e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        tagged_data = parallel(delayed(get_tagged_doc)(doc, i) for i, doc in enumerate(tqdm(docs_w_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ec1c6df-d2ad-4b74-a822-148c9b9dd11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 40\n",
    "window = 3\n",
    "min_count = 20\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6af7b726-25d9-4cc4-a0e1-d1b8ade47d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, epochs=epochs, seed=11, workers=8)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a90c82ab-96d6-429f-b4d9-3e5a378b5614",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('doc2vec_stemmed.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d17a7-f08e-4c8c-a24d-62f18d249665",
   "metadata": {},
   "source": [
    "## new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4caa2c7d-2257-4412-b630-f845890e292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files_cropped/doc_with_text_for_similarity.pickle', 'rb') as f:\n",
    "    docs_w_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb87bd54-07db-4b2c-956d-c6216f819637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    words = [w.strip(punct) for w in word_tokenize(text.lower())]\n",
    "    words = [stemmer.stem(w) for w in words if w not in stopwords and w != '']\n",
    "    return words\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = [w.strip(punct) for w in word_tokenize(text.lower())]\n",
    "    words = [morph.parse(w)[0].normal_form for w in words if w not in stopwords and w != '']\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0c63d1a-b9a1-470c-b305-edea2779e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from uuid import UUID\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "collection_name = \"gosts_titles2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1486f49-b78b-453b-8a02-4fd6440a5b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "punct = '!\"#$%&()*\\+,-\\./:;<=>?@\\[\\]^_`{|}~„“«»†*\\—/\\-‘’'\n",
    "stemmer = SnowballStemmer('russian')\n",
    "stopwords = stopwords.words('russian')\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e47d6cc-e9b3-47fc-9c19-cbc6b8a47c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_doc2vec = Doc2Vec.load('doc2vec_stemmed.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034550d5-b1a2-4008-8851-893876b7c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_list(l):\n",
    "    for i, el in enumerate(l):\n",
    "        if len(el) == 2:\n",
    "            print(f\"{i+1}. {el[0]} {el[1]:.2f}\")\n",
    "        else:\n",
    "            print(f\"{i+1}. {el[0]}. {el[1]} {el[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3302ed9e-b0e3-4ad0-b96f-74245303b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = 'gosts_titles2'\n",
    "collection_name2 = 'gosts_longformer'\n",
    "collection_name3 = \"gosts_mean_chunks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55521119-51d9-4cfb-baa4-4a374d732f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar(doc):\n",
    "    print(f\"Document with title {doc['title']}\\n\")\n",
    "    tokens = stem_text(doc['text'])\n",
    "    inferred_vector = model_doc2vec.infer_vector(tokens)\n",
    "    similar_docs = model_doc2vec.dv.most_similar([inferred_vector], topn=11)\n",
    "    similar_docs = [(docs_w_text[int(doc_id)]['title'], score) for doc_id, score in similar_docs[1:]]\n",
    "    print('Doc2vec:')\n",
    "    pp_list(similar_docs)\n",
    "\n",
    "    vec = np.array(\n",
    "        client.scroll(\n",
    "            collection_name,\n",
    "            scroll_filter=models.Filter(\n",
    "                must=[\n",
    "                    models.FieldCondition(key='gost_number', match=models.MatchValue(value=doc['gost_number']))\n",
    "                ]\n",
    "            ),\n",
    "            with_vectors=True\n",
    "        )[0][0].vector\n",
    "    )\n",
    "\n",
    "    points = client.search(\n",
    "        collection_name,\n",
    "        vec,\n",
    "        limit=11\n",
    "    )\n",
    "    res = [(point.payload['title'], point.score) for point in points[1:]]\n",
    "    print('\\nE5:')\n",
    "    pp_list(res)\n",
    "\n",
    "    vec = np.array(\n",
    "        client.scroll(\n",
    "            collection_name2,\n",
    "            scroll_filter=models.Filter(\n",
    "                must=[\n",
    "                    models.FieldCondition(key='gost_number', match=models.MatchValue(value=doc['gost_number']))\n",
    "                ]\n",
    "            ),\n",
    "            with_vectors=True\n",
    "        )[0][0].vector\n",
    "    )\n",
    "\n",
    "    points = client.search(\n",
    "        collection_name2,\n",
    "        vec,\n",
    "        limit=11\n",
    "    )\n",
    "    res = [(point.payload['title'], point.score) for point in points[1:]]\n",
    "    print('\\nLongformer:')\n",
    "    pp_list(res)\n",
    "\n",
    "    vec = np.array(\n",
    "        client.scroll(\n",
    "            collection_name3,\n",
    "            scroll_filter=models.Filter(\n",
    "                must=[\n",
    "                    models.FieldCondition(key='gost_number', match=models.MatchValue(value=doc['gost_number']))\n",
    "                ]\n",
    "            ),\n",
    "            with_vectors=True\n",
    "        )[0][0].vector\n",
    "    )\n",
    "\n",
    "    points = client.search(\n",
    "        collection_name3,\n",
    "        vec,\n",
    "        limit=11\n",
    "    )\n",
    "    res = [(point.payload['title'], point.score) for point in points[1:]]\n",
    "    print('\\nChunks mean:')\n",
    "    pp_list(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f14c2a8c-03a0-4db9-99d3-fa0229a6a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# интересные доки: 2500, 2623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "413c6e23-1cb4-4f92-911b-6c97f9419ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document with title Генераторы нейтронов. Типы и параметры\n",
      "\n",
      "Doc2vec:\n",
      "1. Генераторы нейтронов. Метод измерения потока быстрых нейтронов 0.75\n",
      "2. Глина бентонитовая для тонкой и строительной керамики. Методы определения показателя адсорбции и емкости катионного обмена 0.68\n",
      "3. Индикаторы знакосинтезирующие. Методы измерения яркости, силы света, неравномерности яркости и неравномерности силы света 0.68\n",
      "4. Детекторы ионизирующих излучений сцинтилляционные. Общие положения по методам измерений сцинтилляционных параметров 0.67\n",
      "5. Индикаторы знакосинтезирующие. Методы измерения времени готовности 0.67\n",
      "6. Источники света искусственные. Метод определения плотности потока энергии ультрафиолетового излучения 0.65\n",
      "7. Приборы электронно-лучевые приемные. Метод измерения неравномерности яркости свечения экрана 0.64\n",
      "8. Реактивы. Свинец (II) уксуснокислый 3-водный. Технические условия 0.64\n",
      "9. Фотоумножители. Методы измерения спектральной анодной чувствительности 0.64\n",
      "10. Концентрат вольфрамовый. Метод определения свинца 0.63\n",
      "\n",
      "E5:\n",
      "1. Генераторы нейтронов. Общие технические требования 0.93\n",
      "2. Игнитроны. Основные параметры 0.90\n",
      "3. Генераторы нейтронов. Метод измерения потока быстрых нейтронов 0.90\n",
      "4. Генераторы пьезоэлектрические. Основные параметры 0.89\n",
      "5. Газотроны импульсные. Основные параметры 0.89\n",
      "6. Генераторы пьезоэлектрические. Система параметров 0.89\n",
      "7. Установки парогазовые. Типы и основные параметры 0.88\n",
      "8. Трансформаторы импульсные. Основные параметры 0.88\n",
      "9. Транзисторы биполярные и полевые. Основные параметры 0.88\n",
      "10. Изоляторы оптические. Типы и основные параметры 0.88\n",
      "\n",
      "Longformer:\n",
      "1. Трансформаторы силовые масляные общего назначения. Допустимые нагрузки 0.90\n",
      "2. Колеса зубчатые цилиндрические мелкомодульные прямозубые и косозубые. Типы. Основные параметры и размеры 0.89\n",
      "3. Сердечники кольцевые из магнитомягких ферритов. Основные размеры 0.88\n",
      "4. Элементы штампуемых деталей. Конструкция и размеры 0.88\n",
      "5. Качество продукции. Статистический приемочный контроль по альтернативному признаку. Случай недопустимости дефектных изделий в выборке 0.88\n",
      "6. Основные нормы взаимозаменяемости. Допуски расположения осей отверстий для крепежных деталей 0.87\n",
      "7. Насосы поршневые и плунжерные. Основные параметры и размеры 0.87\n",
      "8. Диоды полупроводниковые. Основные параметры 0.87\n",
      "9. Стойки установочные крепежные круглые с лысками и резьбовыми отверстиями. Конструкция и размеры 0.87\n",
      "10. Фильтры пьезоэлектрические производственно-технического назначения и для бытовой радиоэлектронной аппаратуры. Основные параметры 0.87\n",
      "\n",
      "Chunks mean:\n",
      "1. Генераторы нейтронов. Общие технические требования 0.96\n",
      "2. Генераторы нейтронов. Метод измерения потока быстрых нейтронов 0.95\n",
      "3. Компенсаторы и уплотнения сильфонные металлические. Общие технические условия 0.95\n",
      "4. Приборы полупроводниковые силовые единой унифицированной серии. Общие технические условия 0.95\n",
      "5. Устройства исполнительные двухседельные средних расходов ГСП. Типы и основные параметры 0.95\n",
      "6. Соединители низкочастотные на напряжение до 1500 В цилиндрические. Основные параметры и размеры 0.94\n",
      "7. Трансформаторы силовые масляные общего назначения. Допустимые нагрузки 0.94\n",
      "8. Электротехника. Буквенные обозначения основных величин 0.94\n",
      "9. Надежность в технике. Метод последовательных испытаний 0.94\n",
      "10. Система оценки соответствия в области использования атомной энергии. Оценка соответствия в форме контроля. Химический состав наплавленного металла (металла шва) 0.94\n"
     ]
    }
   ],
   "source": [
    "get_similar(docs_w_text[1101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7002519-69dc-41d7-8b54-617c3226748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f968b4a-17b2-48d7-a476-2d7d5c53c2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec:\n",
      "1.0\n",
      "doc2vec:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('doc2vec:')\n",
    "print(\n",
    "    ndcg_score(\n",
    "    [[2, 3]],\n",
    "    [[0.0, 1.0]],\n",
    "    k=5\n",
    ")\n",
    ")\n",
    "print('doc2vec:')\n",
    "ndcg_score(\n",
    "    [[1, 2]],\n",
    "    [[0.0, 1.0]],\n",
    "    k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec60c48c-d2d9-4f88-a039-59827bf19ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files_cropped/longformer_embeddings.pickle', 'rb') as f:\n",
    "    lf_embs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b872e19a-e33b-4414-8c31-bc2fc373ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name2 = 'gosts_longformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6342db90-a4a7-43a5-8785-e86c2ef51356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_collection(\n",
    "    collection_name=collection_name2,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=312,\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bdfe422-3701-441c-9e30-bb2968a51ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_collection(\n",
    "        collection_name2,\n",
    "        ids=list(range(1, len(docs_w_text)+1)),\n",
    "        payload=[\n",
    "            {\n",
    "                'gost_number': doc['gost_number'],\n",
    "                'title': doc['title']\n",
    "            }\n",
    "            for doc in docs_w_text\n",
    "        ],\n",
    "        vectors=[np.array(vec, dtype=np.float64) for vec in lf_embs],\n",
    "        parallel=1,\n",
    "        max_retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23417a96-6674-42a1-9c2b-06e20527437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/splitted_texts_query.pickle', 'rb') as f:\n",
    "    chunked_texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25562a37-9f09-4401-b37a-7f52b6679035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e5cac7fbfd418398943366fac64bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert chunked_texts[-1].metadata['doc_id'] == len(docs_w_text) - 1\n",
    "for i, doc in enumerate(tqdm(docs_w_text)):\n",
    "    chunk_embs = [chunk.metadata['embedding'] for chunk in chunked_texts if chunk.metadata['doc_id'] == i]\n",
    "    doc['chunk_embedding_mean'] = deepcopy(np.mean(chunk_embs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ff3ae03-bdbf-405f-9a30-c3b4a5b918f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name3,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=1024,\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f204b10-b2c1-4aa7-8d34-ba3762a0fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_collection(\n",
    "        collection_name3,\n",
    "        ids=list(range(1, len(docs_w_text)+1)),\n",
    "        payload=[\n",
    "            {\n",
    "                'gost_number': doc['gost_number'],\n",
    "                'title': doc['title']\n",
    "            }\n",
    "            for doc in docs_w_text\n",
    "        ],\n",
    "        vectors=[doc['chunk_embedding_mean'] for doc in docs_w_text],\n",
    "        parallel=1,\n",
    "        max_retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53f7db8e-8026-414f-96a9-c6d1a9ef3a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i, doc in enumerate(docs_w_text) if 'Язык программирования' in doc['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d148a2-34e3-4268-bb2b-bf2bc520a4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "681a3d36-089a-4a61-90e6-822d130368f1",
   "metadata": {},
   "source": [
    "# ids from mongo to gosts_chunks_mean qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beaa738d-fdb9-426d-ae76-13e243dcfa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "def get_database():\n",
    "   # Provide the mongodb atlas url to connect python to mongodb using pymongo\n",
    "   CONNECTION_STRING = \"mongodb://localhost:27017\"\n",
    "   # Create a connection using MongoClient. You can import MongoClient or use pymongo.MongoClient\n",
    "   client = MongoClient(CONNECTION_STRING)\n",
    "   # Create the database for our example (we will use the same database throughout the tutorial\n",
    "   return client['GOSTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1db30d0-bf66-4353-8538-59b5b75c9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = get_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d633d1-340d-42ce-a3cb-91b734626057",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_collection = db.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c5e87d8-1faa-464d-83a1-6092c30a5289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from uuid import UUID\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "collection_name = 'gosts_chunks_mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a585cbdd-cdab-478d-a221-4740c1d95272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3518"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for doc in doc_collection.find():\n",
    "    s += 1\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e0921-0023-4fe0-a836-742ec4a229f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## returning lost document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35050f5b-fe38-4c04-a4aa-519ddf8fd063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dataset/docs_before_db.pickle', 'rb') as f:\n",
    "    docs_before_db = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9f5dadb-df9c-4b8d-a65a-e8276ca22eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adc396dfa2b43c68e452517a496de6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "3518",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m els \u001b[38;5;241m=\u001b[39m doc_collection\u001b[38;5;241m.\u001b[39mfind({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgost_number\u001b[39m\u001b[38;5;124m'\u001b[39m: doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgost_number\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(els)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(i)\n",
      "\u001b[1;31mValueError\u001b[0m: 3518"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(tqdm(docs_before_db)):\n",
    "    els = doc_collection.find({'gost_number': doc['gost_number']})\n",
    "    if len(list(els)) == 0:\n",
    "        raise ValueError(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa7df7a7-6828-4d93-95c2-98838aa46e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertOneResult(ObjectId('663fa9788275d659b2aa3caf'), acknowledged=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_collection.insert_one(docs_before_db[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1069122f-3e96-403f-b19c-1be78d60a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "del docs_before_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7925ca-3d2d-4179-b0a5-ba285bc64c22",
   "metadata": {},
   "source": [
    "## rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facf95d6-93b3-4d44-9e82-89d01fabd232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(id: str):\n",
    "    return str(UUID(str(id) + '0' * 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65b9280a-ca47-4c39-8f1e-8a4aaa9ddfed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c32568d2f604236908f1336bb9e3d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = []\n",
    "for doc in tqdm(doc_collection.find(), total=3519):\n",
    "    doc_id = get_id(doc['_id'])\n",
    "    res = client.scroll(\n",
    "        collection_name,\n",
    "        scroll_filter=models.Filter(\n",
    "            must=[\n",
    "                models.FieldCondition(key=\"gost_number\", match=models.MatchValue(value=doc['gost_number'])),\n",
    "            ]\n",
    "        ),\n",
    "        with_vectors=True,\n",
    "        with_payload=True\n",
    "    )[0]\n",
    "    assert len(res) == 1\n",
    "    docs.append({\n",
    "        'id': doc_id,\n",
    "        'gost_number': doc['gost_number'],\n",
    "        'title': doc['title'],\n",
    "        'vector': deepcopy(res[0].vector)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a99ccb7-e9f5-49ab-961d-45c4b43affc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name2 = 'gosts_mean_chunks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e5b6411-a551-4a6e-a1be-aa1c5b769158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_collection(\n",
    "    collection_name=collection_name2,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=1024,\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7a10d42-7cc1-49ce-bbfa-d96d1fb056ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_collection(\n",
    "        collection_name2,\n",
    "        ids=[doc['id'] for doc in docs],\n",
    "        payload=[\n",
    "            {\n",
    "                'gost_number': doc['gost_number'],\n",
    "                'title': doc['title']\n",
    "            }\n",
    "            for doc in docs\n",
    "        ],\n",
    "        vectors=[doc['vector'] for doc in docs],\n",
    "        parallel=1,\n",
    "        max_retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5dc44-633d-45b9-b29e-87d5212292a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a6880d6-385a-41b6-971f-f5f41816502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c65eff-42d1-4c5d-bb56-fce69856b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('models/multilingual-e5-large/tokenizer')\n",
    "session = ort.InferenceSession('models/multilingual-e5-large/model/multilingual-e5-large.onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8ea3e27-04bc-4ea6-a958-8bd0bec41215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts: list) -> torch.Tensor:\n",
    "    ort_inputs = ort_tokenize(texts, tokenizer)\n",
    "    ort_outs = session.run(None, ort_inputs)\n",
    "    embeddings = average_pool(ort_outs, ort_inputs, normalize=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a0d902-dbbe-46ae-a7ac-b494f0a26138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ort_tokenize(texts: list, tokenizer) -> dict:\n",
    "    inputs = tokenizer(texts, return_tensors=\"np\", max_length=512, padding=True, truncation=True)\n",
    "    ort_inputs = {\n",
    "        \"input_ids\": inputs['input_ids'].astype(np.int64),\n",
    "        \"attention_mask\": inputs['attention_mask'].astype(np.int64),\n",
    "    }\n",
    "    return ort_inputs\n",
    "\n",
    "def average_pool(ort_outs: np.ndarray, ort_inputs: dict, normalize: bool = True, return_tensors: str = 'np') -> torch.Tensor:\n",
    "    last_hidden_states = torch.Tensor(ort_outs[0])\n",
    "    attention_mask = torch.Tensor(ort_inputs['attention_mask'])\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    embeddings = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "    if normalize:\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    if return_tensors == 'np':\n",
    "        embeddings = np.array(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4965f7a2-cde6-4865-9045-8a43b5a3a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_collection(\n",
    "    'gosts_titles2',\n",
    "    ids=[get_id('663fa9788275d659b2aa3caf')],\n",
    "    payload=[\n",
    "        {'gost_number': 'ГОСТ 20809-75', 'title': 'Патроны охотничьи 9х53. Типы и основные размеры'}\n",
    "    ],\n",
    "    vectors=get_embeddings('Патроны охотничьи 9х53. Типы и основные размеры')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b852086-e956-46de-8718-1968e0e2c844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
